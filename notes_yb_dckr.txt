
fetched version with 2 docker commands (start deamon first) :
docker pull yugabytedb/yugabyte:2.15.3.2-b1
docker run -d --name yugabyte  -p7000:7000 -p9000:9000 -p5433:5433 -p9042:9042 yugabytedb/yugabyte:2.15.3.2-b1 bin/yugabyted start --daemon=false --ui=false


-- new version:
docker run -d --name yugabyte  -p7000:7000 -p9000:9000 -p5433:5433 -p9042:9042  7b640a82b04b  bin/yugabyted start --daemon=false --ui=false
7b640a82b04b

-- volume mapped
docker run -d --name yb219b \
  -p7000:7000 -p9000:9000 -p5433:5433 -p9042:9042 \
  -v ~/var/yb_datb:/home/yugabyte/yb_data \
  yugabytedb/yugabyte:latest bin/yugabyted start \
  --base_dir=/home/yugabyte/yb_data --daemon=false

to start the shell
docker exec -it yugabyte /home/yugabyte/bin/ysqlsh

shell didnt work, but psql did work:
psql -h localhost -p 5433 -U yugabyte

note: that shell from docker is inside the container ? 
does it pick up scripts from laptop ?


utils: you can shart bash and sh ..
docker exec -it yugabyte /bin/bash


the collect_logs was not found


in case of non-start: remove files in
/tmp/.yb... 
:wq


-- - -- notes franck -- -- -- 

range or hash

index in same or different tablet : how to measure impact ? 
index: overload? add extra fields to favour index-only lookup ? 

index: the first field is sharded by defulkt and next fieds are ordered.

scan is seek-next, seek-next.. and LSM tree. need to read up on LSM


IP or hostname: 
 - why does YB use IP, notably in the masterlist I cant get rid of IPs


PG-version: 
 - time to upgrade.. why the wait, what are the bottlenecks.


- -- -- 

try creating many tables...
using pgplsql

#!/bin/ksh
#
# yb_multi.sh: try creating a multi node yb-cluster in docker
#

docker network create yb_net

# start 1st master, call it node1, netowrk addres: node1.yb_net
docker run -d --network yb_net  \
  --hostname node1 --name node1 \
  -p15433:15433 -p5433:5433     \
  -p7001:7000 -p9001:9000       \
  yugabytedb/yugabyte           \
  yugabyted start --background=false --ui=true

# found out the hard way that a small pause is beneficial
sleep 15

#now add nodes..
docker run -d --network yb_net  \
  --hostname node2 --name node2 \
  -p7002:7000 -p9002:9000       \
  yugabytedb/yugabyte           \
  yugabyted start --background=false --join node1.yb_net

sleep 15

docker run -d --network yb_net  \
  --hostname node3 --name node3 \
  -p7003:7000 -p9003:9000       \
  yugabytedb/yugabyte           \
  yugabyted start --background=false --join node1.yb_net

sleep 15

docker run -d --network yb_net  \
  --hostname node4 --name node4 \
  -p7004:7000 -p9004:9000       \
  -p15434:15433 -p5434:5433     \
  yugabytedb/yugabyte           \
  yugabyted start --background=false --join node1.yb_net

sleep 15

docker run -d --network yb_net  \
  --hostname node5 --name node5 \
  -p7005:7000 -p9005:9000       \
  yugabytedb/yugabyte           \
  yugabyted start --background=false --join node1.yb_net

sleep 15

docker run -d --network yb_net  \
  --hostname node6 --name node6 \
  -p7006:7000 -p9006:9000       \
  yugabytedb/yugabyte           \
  yugabyted start --background=false --join node1.yb_net

# health checks:
docker exec -it node1 yugabyted status 
docker exec -it node2 yugabyted status 
docker exec -it node3 yugabyted status 
docker exec -it node4 yugabyted status 
docker exec -it node5 yugabyted status 
docker exec -it node6 yugabyted status 

# experiment with nodename, zone..
docker run -d --network yb_net  \
  --hostname node7 --name node7 \
  -p7007:7000 -p9007:9000       \
  yugabytedb/yugabyte           \
  yugabyted start --background=false --join node1.yb_net


echo .

echo Scroll back and check if it all workd...
echo .
echo Also verify: 
echo  - connecting cli    : ysqlsh -h localhost -p 5433 -U yugabyte
echo  - inspect dashboard : localhost:15433 
echo  - inspect node3:    : localhost:7003   (and 9003, etc...)
echo . 
echo Have Fun.
echo .
echo .


Nodeneams:  edit, then restart..
/root/var/conf/yugabyted.conf


-- how to prepare a yb-node (automate...):
 - install ybtool.gz, copy to PATH
 - bash-export $MASTERS and some aliases.
 - check yb-admin  
 - 

# 
# yb-profile.sh: useful additions to bash-profile
# add to /root/.bashrc, use do_all.sh ? 
#
export MASTERS=`cat /root/var/conf/yugabyted.conf | grep masters | cut -b25-71 ` 
alias    yba='yb-admin -master_addresses $MASTERS ' 
alias ybuniv='yb-admin -master_addresses $MASTERS get_universe_config '
alias ybmast='yb-admin -master_addresses $MASTERS list_all_masters ' 
alias ybtsrv='yb-admin -master_addresses $MASTERS list_all_tablet_servers '
alias ybtrep='yb-admin -master_addresses $MASTERS list_tablet_servers '
alias ybtbls='yb-admin -master_addresses $MASTERS list_tables  include_db_type include_table_id include_table_type | sed s'/\./\ /g' '

add to profile for container: copy to container + cat to bashrc
docker cp yb_profile.sh node1:/tmp/
docker exec -it  node1  sh -c "cat /tmp/yb_profile.sh >> /root/.bashrc "  

# useful cuts:

# cut out data from yb-admin, find tablet servers:
tsrvrs  | tail -n3 | cut -c1-38,39-49,66-70 
tsrvrs             | cut -c1-33,35-48,65-70,91-99
ybmast | cut -c1-33,35-48,65-70

# listing all tables with yb-admin: db, namespace, table-type (skip catalog), replace the dots by spaces
yb-admin -master_addresses $MASTERS list_tables  include_db_type include_table_id include_table_type | sed s'/\./\ /g' 

# list all tservers that serve a tablet, needs tablet-id
yb-admin -master_addresses $MASTERS  list_tablet_servers 04b26afaf0604684aabba96f3d35f419 


# nodes, masters and servers..
create table ybwr_mstr ( uuid text primary key, hostadd text, status text ) ;
create table ybwr_tsrv ( uuid text primary key, hostadd text, status text, up_secs bigint ) ;

create table ybwr_tabl ( uuid text primary key, hostadd text, status text ) ;

# cut data to file (or stdout) and read it with copy command
 \copy m from 'mast.out' with delimiter ' ';
 \copy m from 'tsrv.out' with delimiter ' ';


# get more info, dsar + yb_stats : get executable from githubmm

./dsar -H localhost -P 9001,9002,9003,9004,9005,9006 -o yb-cpu -i 1 
./dsar -H localhost -P 9001,9002,9003,9004,9005,9006 -o yb-io -i 4 
     
and to investigate entities... 
 ./yb_stats -H localhost -P 7001,7002 --print-entities | more 

./yb_stats -H localhost -P 7001,7002  --print-masters  

for failover, try  using an URI
ysqlsh postgresql://yugabyte@localhost:5434,localhost:5433,localhost:5432,localhost:5431 


worker node: nodeX 
- runs stand alone 1 db
- basrhc: same aliaes, but notably connecting to the cluster, via other nodes.
- failover in ysql-uri
- script to insert/sec.. : do_fill.sql
  add connected-host to filler


todo:
 - ensure pg-startup by removing sockets..
  rm -rf /tmp/.yb.*


scale-up to handle peak-usage..
 - Looking to scale up (not quite out) without extra nodes or shards
 - Can other nodes Read from shared or borrowed storage ? (e.g. read data without having to replicate data to read-only nodes)
 - 

next:
 - run container with bash, as vritual server.
 - test join/stop/start of nodes, from sever-level.
 - use hostnames ?
 - test procstat


Hi all,

TL;DR: re-starting a stopped node fails with an error that doesnt make sense to me.

I have a difference in behaviour between two (several) container-versions.

So far I've used :
# choose an image
# YB_IMAGE=yugabytedb/yugabyte:latest
# YB_IMAGE=yugabytedb/yugabyte:2.19.0.0-b190
# YB_IMAGE=yugabytedb/yugabyte:2.19.3.0-b140
  YB_IMAGE=yugabytedb/yugabyte:2.20.1.0-b97

My last "working" version is 2.19.0.0-b190
None of the higher version work, and they produce the same error/phenomenon:
ON the stop+restart of a node, it tries to retrieve the master-info from node2, it retrieves the correct IP, but it refuses to join, and reports that a node is already running on that address..

it means that after cluster-create (with success), I permanently loose every node I shut down. 
The tail from a yugabyted.log:
[yugabyted start] 2024-01-26 13:47:26,930 INFO:  | 0.0s | Running yugabyted command: '/usr/local/bin/yugabyted start --background=true --join node4'
[yugabyted start] 2024-01-26 13:47:26,930 INFO:  | 0.0s | cmd = start using config file: /root/var/conf/yugabyted.conf (args.config=None)
[yugabyted start] 2024-01-26 13:47:26,932 INFO:  | 0.0s | Found directory /home/yugabyte/bin for file openssl_proxy.sh
[yugabyted start] 2024-01-26 13:47:26,932 INFO:  | 0.0s | Found directory /home/yugabyte/bin for file yb-admin
[yugabyted start] 2024-01-26 13:47:26,934 INFO:  | 0.0s | Fetching configs from join IP...
[yugabyted start] 2024-01-26 13:47:26,935 INFO:  | 0.0s | Trying to get masters information from http://node4:9000/api/v1/masters (Timeout=60)
[yugabyted start] 2024-01-26 13:47:26,941 DEBUG:  | 0.0s | Tserver node4 returned the followingmaster leader 172.20.0.2.
[yugabyted start] 2024-01-26 13:47:26,943 ERROR:  | 0.0s | ERROR: A node is already running on 172.20.0.2, please specify a valid address.
For more information, check the logs in /root/var/logs
/end-tail.

I seems obvious: the starting process thinks it is on the same IP as the previously started node, but.. it isnt.

The first node in the cluster is started, using:
# start 1st master, call it node2, network address: node2.yb_net
docker run -d --network yb_net  \
  --hostname node2 --name node2 \
  -p15432:15433 -p5432:5433     \
  -p7002:7000 -p9002:9000       \
  -v /Users/pdvbv/yb_data/n2:/root/var  \
  $YB_IMAGE                             \
  tail -f /dev/null

docker exec node2 yugabyted start --background=true --ui=true

All subsequent nodes are started, with instrution to join:

#now add nodes..
docker run -d --network yb_net  \
  --hostname node3 --name node3 \
  -p15433:15433 -p5433:5433     \
  -p7003:7000 -p9003:9000       \
  -v /Users/pdvbv/yb_data/n3:/root/var  \
  $YB_IMAGE                             \
  tail -f /dev/null

docker exec node3 yugabyted start --background=true --join=node2


This way, I can succesfully start a 7 node cluster and run whatever tests I want.
But .. on stop-start of any node, it refuses, and produces the error above....


Any Idea ?
What is diff for join=node in the later versions?
What options should I give yugabyted in newer versions ? 


Hello,

TL;DR: can a node/master/tserver in a cluster use hostname, rather then IP because the IP can be different restart.? 

I am running a 7-node cluster, currently from docker containers.
The image I use is: YB_IMAGE=yugabytedb/yugabyte:2.20.1.3-b3

the stoping of containers (nodes) is either done by "killing container", or using yugabyted stop.
starting is done by "yugabyted start"  (no options needed? as the join-node is specified in yugabyted.conf)

By stopping and starting containers, I can not guaranteel each container (node) always has the same IP.
This seems to be a problem becasue the yugabyted.conf stores the masterlist as IP:port, and not as nodename:port.

my questions are:
 - is it supported that host or node with YB master or tserver on it is re-started with same hostname but another IP ? 
 - is yugbyted.conf the only place where IPs or nodenames are kept+stored ? 


Testing:
 - explain plan : 1 table, 1-n tables (in which direction to search)
 - when to use hash, when to use range
 - colocation : for smal tables
 - how many tablets ? 2, 8 ? 
