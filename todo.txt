
generic todo list

 - put ybx_logging in separate user, database + tablespace ? 
    - test by re-creating + rollout.. user goes in ... 
      do_ashloop.sh, do_snap.sh, do_ar.sh, collect_ash.sh, p_get.sh
  
 - devise way to snapshot a problem-interval:
    - stop automatic collection
    - cleanout: delete all ybx_logg data
    - on 1 node: do_snap.sh: gather meta-data, parent tables..
    - run problem-program
        - if longer run: collect data every 5 (?) min to not lose buffered recors
    - when run finished: collect (final) from all nodes 
    - do_snap.sh (do we need this ? ): it notably provides tsrv_log and mast-log, uptimes...
    - generate rr and related records and views (mk_rr.sql) 
    - backup or export all the data to 1 file... for viewing elsewhere.

 = p_get.sh : nice overview of session, can use pg_bench
    - work on improving, list rr + qry + events in 1 set. order by rr.
    - list most occuring queries. (for oltp)
    - add 8-char of wait-aux (to shnow various tablets, dont duplicat table-name
    - do short bencmark + output.
    - verify : interval, sql-trace
    - vary nr of tablets to get more inter-node traffic?

 - reduce poll-effort: datb + evnt can be 30min things per tsrv..
    - host-log and filep-scraping can be reduced ? 
    - interval 30min, or after restart (uptime of tsrv)

 - improve session_detection from ash (remove 1=0 ? ) 
    - needs a way to correct data for sessions found too-early..
      update usesysid where null, IF found.. 
      update backend_start if earlier timestamp found  

 - include rr and rr-qury into ashy ? 
    - this implies to extract session from ash as well..
    => much more effort ? 

 - check root-req and make profile of query:
    which sessions, how long, and what items in activity-list.. 
      given the q-id: sessions, rr-s, and chronological-path over nodes..

 - profile known session with long count and function-calls ? 
    p_get.sh.. can be improved

 - log pg_locks as well ? notably thew yb-json values ?

 - divide graph-dashboards into ash + snapshot, 
    missing data will give indication of working components.

 - take slow qrys from ash, and create indexes: inserts-first, do_ar.sh 2nd.
    - notaby the grafana sql.. can be better ! 

 - ysql_bench doesnt seem to work ? (fixed)
    - try from pg container, connect to yb ? 
    => workaround, see mk_ybash.sql

 - remove nodes from cluster, tservers and masters
 - split ash-script into separate repostiories on github

 - Next: investigate multi-region and replication.: 
    seems simple replication

 - investigate ports 12000 and 13000 : curl ? 
    => now mapped.

 - remove call to get_host and get_tsrv if possible
    notably  do_ash.sql: see example in do_snap.sh

 - [optionn] add sar-idle number to host-log: last nr of tghis ..
   sar 2 1 | tail -n1 | sed 's/ \+/\|/g' 

 - log an aas; avg-act-ses, find baseload ? 
 - need solution to faster push

 - views over ash-data : smaller tables, more info in view (notably host, query, session)

 - test fast ingest.. ? with at least 1 index on date ? 
  - with host + now() per deflt-per-recors
  - with host + now() from with-clause
  - with 1-per-row, with 100, with 1000 per ins..
  - use now() versus clock_timestamp()
  - insert x records... ? 10M ? 
  - insert from 1 or more nodes..
  - insert into 1 or 4 tablets

 
todo:
 - qury: find sessions, root-req, timings..
    - now to get plan from memory ? 
    - see below sql
 - make views with crosstab (?) for graphs to adjust for nr servers.
 - quick report on counts: how much data was collected..: chk_ash.sql
 - Master data: detect new combinations for univ_mst, host_mst, mast_mst and tsrv_mst : 
    - it seems to mix up mast and tsrv ? possibly the delete kv doesnt work
 - add event-listing to main scripts: done, needs testing: check.. ? 
 - tsrv_mst has to act as decoding between uuid and host.
    - snap_id ( = also found_dt )
    - host (where it runs, not where it is found!)
    - pk: host + tsrv , the snap_id is  just a time-indicator.
 - tables and tablets for colocations: partly done, needs Testing !

next
 - log more server-data: use script, function.. use unames + yb_func().. just log /180sec
 - set grafana open
 - run some load..
 - re-edit scripts + instructions to deploy:
    - yb_init.sql (still needed ?) 
    - mk_ybash.sql : move all to mk_yblog.sql
      - do_stuff.sh, do_ashloop.sh, do_sadc.sh
    - yb_osmetrics.sql
    - unames.sql + sh (all nodes)
    - do_snap.sh (only 1 node, needs separate loop-script.) 

need a way to read key=values into table ybx_ingest: (id-seq, host, key, value )... all text..
that way, reading from varz or just files is easier..
=> done in unames.sql in do_ash.sql and  unames.sh

get master info, etc: 
ybmast | expand | tail -n3 | sed 's/ \+/\|/g' 


To log master-server config, have 1 node collect:: 
1. determine snap_id + log_dt
2. scrape master-info:  uuid + node + port + STATUS + leader/follower
3. scrape tserver-info:  uuid + node + port + heartbeat + STATUS + leader/follower
4. log tserver_metrics for all nodes yb_server_metrics()

on each node: collect t-server-info
4. add to uname or ashloop: use last snapshot to find uuid for node, use uuid to select from yb_heap_stats()

echo "local_tserver=" `ybmast | grep $HOSTNAME | cut -c1-33`


-- work on here.. ybx_tsrv_log (how to find uuid?)
other sources of data: for tserver

-- heap is local to server, must be in host_log, via unames.sql
select * from yb_heap_stats();  

-- metrics can be part of snapshot.. 
select * from yb_servers_metrics () ; 
done, in do_snap

select tl.host, tm.status, tm.error
, (tm.metrics::json->>'memory_free')::bigint/1024/1024      as mem_free_mb
, (tm.metrics::json->>'memory_total')::bigint/1024/1024     as mem_total_mb
, (tm.metrics::json->>'memory_available')::bigint/1024/1024 as mem_avail_mb
, (tm.metrics::json->>'tserver_root_memory_soft_limit')::bigint/1024/1024 as tserver_root_memory_soft_limit_mb
, (tm.metrics::json->>'tserver_root_memory_consumption')::bigint/1024/1024 as tserver_root_memory_consumption_mb
, (tm.metrics::json->>'cpu_usage_user')::real  as cpu_usage_user 
, (tm.metrics::json->>'cpu_usage_system')::real  as cpu_usage_system 
, tm.uuid
from ybx_tsrv_log tl,  yb_servers_metrics () tm
where 1=1 
-- and tl.snap_id = 4801
and tl.tsrv_uuid = tm.uuid
order by tl.host, tl.snap_id desc ; 


select * from pg_catalog.yb_mem_usage_sql() ;

next; log data and display in graphana.
:


todo - searching for SQL.
 - top-SQL with duration via RR
 - top-SQL with count-events.., count per SQL or TSrv
 - top SQL per node (with count-events)
 - top SQL, with most top-PIDs : many small ones
 - top SQL, with Least PIDs: repeated SQL, or long running SQL


-- regarding the metrics on ports 7000 and 9000..
- metrics are global for process of t-server/master (but what about the postgres processes): 
- postgres-processes, client-connetions: local, need more precise info if avaiable 
  client-connections, are the "most interesting" to collect metrics from  ? 

- prometheus data: many nrs, 90% in my case, are 0 ( I checked with awk...)
- even from  non-zero numbers, a lot is "available", but not necesarily "useful".
- try asking questions... how many client-conn.. how many top-queries, how many dependent qries..
- can we measure "inside a session" ?  or have a session(process, pid) save/report its metrics ?
  And even if we can peek inside processes or structures: is the data relevant + useful?
- the pg_stat_stmnts are "aggregates" over all sessions that did the query
  But the SEssion, the Client, needs to know what he was waiting for. 
- Ideally, some timing per root-request is wanted ? 
    (can be derived, via min/max of sample time, but with single-second accuracy?)
- many metrics have count + time.. but always cumulative.. 
- is there count of nr of outgoing calls per session or query? 
- how can a session report time-spent, possibly broken down to time-spent-per-root-request, or per qury..

- ASH wise: the 1sec interval seems to miss lot of short/fast queries.. 
  is pg_stat_% a better place to look for metris ? 

curl from ports : do_curls.sh..
set "|" as separator: cat node2_9000.out | sed 's/{/\|/g' | sed 's/} /\|/g' | sed 's/ /\|/g'
filter out the zeros: awk -F'|' '$3 != 0 {print $1, $3 }'

Big Question : which metrics are relevant to session or to root-request.. 


Regarding Datamodel... 

Aim is currently to find heavy processes and heavy queries.
Spotting underlying tabblets / tables may/should follow from that.
But other may be more interested to dive directly into the storage-layer
(as thew metrics on port 9000 seem to focus on rocksdb)


Central, I think, is the "session". identified from from pg_stat_activity

Session, a postgres-process with a pid on a host, belongs to a tserver, on a host
Session also links to an "application" or some component that is initiating work on the system.
table: ybx_sess_mst: try to catch the master record for every session.

My main source for session-data is pg_stat_activity.

The Session is then linked to ASH, where the work is "polled" from yb_act_sess_hist:
table: ybx_ashy_log : data logged from yb_active_session_history

ASH records are local to a tserver, 
and belong to a session or parent_process, possibly via root-request
But that RR is hard to "spot": does it belong to the Session, to the tsrver, or to the query? 
possibly the link of the three. TBD.

On the Sessions, We can regularly "poll" logging data out of a session from pg_stat_activity: 
table : ybx_sess_log, regular polled data from pg_stat_activity


Looking upwards from session, the session is initiated at a t-server, 
and we can regularly capture (scrape) data from tserver 
via yb-admin and function yb_server_metrics(). And also via port 9000/prometeus
table: ybx_tsrv_mst : 1 record per tserver
table: ybx_tsrv_log : regular polled data, e.g. memory, threads, cpu...

Similarly, we can capture data for universe and yugabyte-master
table: ybx_mast_mst: 1 record per master
table: ybx_mast_log: polled data from yb-admin

Back to Sessions, and queries.
Both the session pg_stat_activity) and the ASH-record contain a query_id.
We can capture query-data from pg_stat_statements, resulting in :
ybx_qury_mst : just signal the existance of the qry, and act as masater j+ linking-pin
ybx_qury_log : capture data per t-server from pg_stat_statements

note that the cummulative data in pg_stat_stmt is not relatable to a session (yet)

Possibly there is a query-plan (explain plan) in there, 
and a query can have muliple of those plans
table: ybx_qury_pln : some as yet unknown way to store a plan...


For storage details, we know ASH acts on tablets, local to the node where the ash is 
ybx_tblt_mst : 1 record per tablet (which will have at least 3replicas... )
ybx_tblt_rep : replica, local to a t-server, 
Note that a tablet_replicat will have "role": leader or follower or... 

Tablets are linked to tables, and becasue of possible "colocation" 
this will require an n:n. Modelling this out: table  + linking entity:
ybx_tabl_mst : master record per table, key is table_uuid, and oid links back to postgres.
ybx_tbl_log  : info from pg_stat_tables, per tsrve3r, per log_dt ?
ybx_tata_lnk : linking table to tablet: just two keys ?

tables will belong to a database, and from pg_database views we can collect info on those:
ybx_datb_mst : 1 record per database, key is datid, the oid inside postgres.
ybx_datb_log : regular polled data, per t-server key is datid + tsrverk + log_dt

The whole reasoning leads to an ERD, and to a primitive script to create the tables.
in real-life, the constraints may have to be left-out 
because of the order in which we can collect the data: details first, meta-data to be derived.

script: attached.
two partial erds: attached..

scripts and functions to collect the data: still in messy development. partly working.

graphana example.. attached.



Request to add Event_ID to the views yb_active_session_history and to yb_event_desc.

The view yb_active_session_history is useful, but in some cases we want to "keep that history" and store it into a table.

There are four text-fields: wait_event, _type, _class and _component.
For storing the data, having 4 text-fields, in 1000s or millions of records, is less efficient. 
Furthermore, from saving a few days of ASH-records, we found there are probably less than 100 unique combinations of 
event, type, class, and component. 
This was confirmed by yb_wait_ev_desc: 47 records in our case. 
Even if multiplied by the possibilities for "wait_event_component" this remains a manageable nr.
Hence the suggestion to add the ID field to both views.
A simple "int" would suffice for storing the ASH data into a table, saving on storage, memory and network-traffic. 

It will therefore be useful if the view yb_wait_event_desc 
will display an ID (wait_ev_id) that can be referred by the records in yb_wait_event_descr.

Note1 that further savings in the size of (storing) the ASH record are possible; a lot of client- and root-req-related info seems to repeat and seems to depend solely on the Root_Req. Separate issue to follow.


Root_request_id.
I am trying to figure out myself what the rr_id actually "means". 

The view yb_active_session_history contains a numbrer of fields that seem to depend on the Root_req_id.


-- notes on grap + helper functions.


drop view ybg_tsrv_rwr; 
create or replace view ybg_tsrv_rwr as 
select sl.log_dt
, t2.rd_psec n2_rds
, t2.wr_psec n2_wrs
, t3.rd_psec n3_rds
, t3.wr_psec n3_wrs
, t4.rd_psec n4_rds
, t4.wr_psec n4_wrs
, t5.rd_psec n5_rds
, t5.wr_psec n5_wrs
from ybx_snap_log sl
   , ybx_tsrv_log t2
   , ybx_tsrv_log t3
   , ybx_tsrv_log t4
   , ybx_tsrv_log t5
where  1=1
 and sl.id = t2.snap_id 
 and sl.id = t3.snap_id
 and sl.id = t4.snap_id
 and sl.id = t5.snap_id
 and t2.host = 'node2'
 and t3.host = 'node3'
 and t4.host = 'node4'
 and t5.host = 'node5'
order by sl.log_dt  ; 

select log_dt, n2_rds, n2_wrs, n3_rds, n3_wrs, n4_rds, n4_wrs from graph_read_write order by log_dt  ; 

drop view ybg_tsrv_cpu; 
create or replace view ybg_tsrv_cpu as 
select sl.log_dt
, t2.cpu_user n2_usr
, t2.cpu_syst n2_sys
, t3.cpu_user n3_usr
, t3.cpu_syst n3_sys
, t4.cpu_user n4_usr
, t4.cpu_syst m4_sys
, t5.cpu_user m5_usr
, t5.cpu_syst m5_sys
from ybx_snap_log sl
   , ybx_tsrv_log t2
   , ybx_tsrv_log t3
   , ybx_tsrv_log t4
   , ybx_tsrv_log t5
where  1=1
 and sl.id = t2.snap_id 
 and sl.id = t3.snap_id
 and sl.id = t4.snap_id
 and sl.id = t5.snap_id
 and t2.host = 'node2'
 and t3.host = 'node3'
 and t4.host = 'node4'
 and t5.host = 'node5'
order by sl.log_dt  ; 


-- better to get tsrv_uuid, include blacklisted tsrvs
with h as ( select ybx_get_host() as host )
 SELECT coalesce ( uuid::uuid, m.tsrv_uuid ) jj
    FROM yb_servers () s, ybx_tsrv_mst m, h
    WHERE s.host = h.host
     and m.host = h.host ;


-- extracting duration form root-request , given SQL 
-- negative means: the rr did other queries first.

-- with givne SQL, find rr of last 900 sec
with rr as ( select ad.root_request_id, min ( sample_time) start_dt  -- collection of rrs...
               from ybx_ashy_log ad 
               where query_id in (-6243711630595800184)
               group by ad.root_request_id
)
select /*al.top_level_node_id, al.root_request_id, */ al.pid
, al.host, al.wait_event, qm.query
, to_char (al.sample_time, 'HH24:MI:SS.MS') as time
--, to_char ( al.sample_time - start_dt, 'SS.MS' ) as ela_ms
, extract ( epoch from   al.sample_time - start_dt ) * 1000 as ela_ms
, al.* 
from ybx_ashy_log  al
, rr rr
, ybx_qury_mst qm
where 1=1
and al.root_request_id = rr.root_request_id
and qm.queryid = al.query_id 
--and query_id in (3121461546181528127, 99.7608093105707391554, -6243711630595800184)
order by al.sample_time, pid  desc ;


better 

-- with givne SQL, find rr of last 900 sec
with rr as ( select ad.root_request_id, min ( sample_time) start_dt  -- collection of rrs...
               from ybx_ashy_log ad 
               where query_id in (-5862996160561394642)
                   or 1=0 --pid = 98191
               group by ad.root_request_id
)
select /*al.top_level_node_id, */ al.root_request_id,  al.pid
, al.host 
, extract ( epoch from   al.sample_time - start_dt ) * 1000 as ela_ms
, al.wait_event
, al.wait_event_aux
, qm.query
, to_char (al.sample_time, 'DD HH24:MI:SS.MS') as time
--, to_char ( al.sample_time - start_dt, 'SS.MS' ) as ela_ms
, al.* 
from ybx_ashy_log  al
, rr rr
, ybx_qury_mst qm
where 1=1
and al.root_request_id = rr.root_request_id
and qm.queryid = al.query_id 
--and query_id in (3121461546181528127, 99.7608093105707391554, -6243711630595800184)
order by al.sample_time , pid  desc
;


psuedo for function:
 - collect the root-reqs  and min(tie) for the query-id,
 - for every root req
    - print session + rr + client_id + client-host + start dt.
    - loop over ash for rr:
      -print elepsed (ms). host, wait-ve, table (if any) (componsnet + type + classe)_

could use temp-table like this:
  origin_host, start_dt, sess, program, pid, datid+datnane, userid_username, rr, on_host, elepsed, event (component, class, type), table-if-known


function could report: 
  - per query (all data from sql): would take additional queries in rr.
  - per query-per-time-window (default: last hr)
  - per root-request: all queries in rr.
  - per session: all rr's per session: top_node + pid [, + startdt.. ]


generate a lot of data.. 
copy seems fastest way to ingest..
to generate 100 lines of 32 bytes..:
for i in {1..100}; do head -c 32 /dev/urandom | base64 | head -c 32; echo; done > input.txt



These folks are On To Something Good...
Yugabyte just released the next version of their PostgreSQL compatible RDBMS, <version here>. 
The top-end code is a Postgres fork, ensuring seamless compatibility 
with the thousands of application out there that now use PostgreSQL.
 
The back-end storage layer is based on RocksDB, and allows for data to be replicated for resilience and sharded for scalability. These features make Yugabyte a Very Interesting player in the area of Distributed and serverless databases. 

Furthermore, YBDB seems to have put a lot of effort into the seamless, non-downtime, upgrade-capability. This should mean that future versions of YB are able to catch up with the "current" releases of Poestgres.

And the support seems to be responsive. Even as I wast just discovering the product 
and doing DEV- and PoC-type of deployments, 
I seemed to get good ansers to my questions from the YB people on their slack-channel.


What I also find important: This database providor is not (yet) linked to any of the large cloud-vendors. 
This should mean that YBDB has no intention of locking you in or "capturing" you onto 1 specific cloud-provider. 
Just like "serverless" liberates the IT department form the vendors of hardware and services, 
the "distributed" processing should liberate us from any cloud lock-in.



Like several other YB-users, I have been waiting for the pg-15 release of YBDB.

Furthermore, Yugabyte now states that future version-upgrades can 
follow at faster rate, which may help them catch up to PG-17.

After using pg-11 and pg-15 for 

and connectivity from all applications that currently use.
o

Including the promise that next releases can follow , now PG-
After observing Yugabyte for a few years, I think they are on the Right Track. The front-end is effectively a Postgres Form, which ensure Excellent Compatibility. The back seems built on RocksDB. The main feature (IMHO) is that this database is now "serverless", meaning you dont know anymore on

I am very curious to see how this concept is going to work out in 
