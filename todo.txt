
generic todo list

 - remove nodes from cluster, tservers and masters
 - split ash-script into separate repostiories on github
 - investigate multi-region and replication.
 - log an aas; avg-act-ses, find baseload ? 
 - create subdir with sh-scripts to run evey N seconds: ashloop, unames, master-log, tserver-log...
 - need solution to faster push

todo:
 - quick report on counts: how much data was collected..
 - Master data: detect new combinations for univ_mst, host_mst, mast_mst and tsrv_mst : 
 - add event-listing to main scripts: done, needs testing
 - has to act as decoding between uuid and host.
    - snap_id ( = also found_dt )
    - host (where it runs, not where it is found!)
    - pk: host + tsrv , the snap_id is  just a time-indicator.
 - tables and tablets for colocations: partly done, needs Testing !

next
 - log more server-data: use script, function.. use unames + yb_func().. just log /180sec
 - set grafana open
 - run some load..
 - re-edit scripts + instructions to deploy:
    - yb_init.sql (still needed ?) 
    - mk_ybash.sql : move all to mk_yblog.sql
      - do_stuff.sh, do_ashloop.sh, do_sadc.sh
    - yb_osmetrics.sql
    - unames.sql + sh (all nodes)
    - do_snap.sh (only 1 node, needs separate loop-script.) 

need a way to read key=values into table ybx_ingest: (id-seq, host, key, value )... all text..
that way, reading from varz or just files is easier..
=> done in unames.sql

get master info, etc: 
ybmast | expand | tail -n3 | sed 's/ \+/\|/g' 


To log master-server config, have 1 node collect:: 
1. determine snap_id + log_dt
2. scrape master-info:  uuid + node + port + STATUS + leader/follower
3. scrape tserver-info:  uuid + node + port + heartbeat + STATUS + leader/follower
4. log tserver_metrics for all nodes yb_server_metrics()

on each node: collect t-server-info
4. add to uname or ashloop: use last snapshot to find uuid for node, use uuid to select from yb_heap_stats()

echo "local_tserver=" `ybmast | grep $HOSTNAME | cut -c1-33`


-- work on here.. ybx_tsrv_log (how to find uuid?)
other sources of data: for tserver

-- heap is local to server, must be in host_log, via unames.sql
select * from yb_heap_stats();  

-- metrics can be part of snapshot.. 
select * from yb_servers_metrics () ; 
done, in do_snap

select tl.host, tm.status, tm.error
, (tm.metrics::json->>'memory_free')::bigint/1024/1024      as mem_free_mb
, (tm.metrics::json->>'memory_total')::bigint/1024/1024     as mem_total_mb
, (tm.metrics::json->>'memory_available')::bigint/1024/1024 as mem_avail_mb
, (tm.metrics::json->>'tserver_root_memory_soft_limit')::bigint/1024/1024 as tserver_root_memory_soft_limit_mb
, (tm.metrics::json->>'tserver_root_memory_consumption')::bigint/1024/1024 as tserver_root_memory_consumption_mb
, (tm.metrics::json->>'cpu_usage_user')::real  as cpu_usage_user 
, (tm.metrics::json->>'cpu_usage_system')::real  as cpu_usage_system 
, tm.uuid
from ybx_tsrv_log tl,  yb_servers_metrics () tm
where 1=1 
-- and tl.snap_id = 4801
and tl.tsrv_uuid = tm.uuid
order by tl.host, tl.snap_id desc ; 


select * from pg_catalog.yb_mem_usage_sql() ;

next; log data and display in graphana.
:


todo - searching for SQL.
 - top-SQL with count-events.., count per SQL or TSrv
 - top SQL per node (with count-events)
 - top SQL, with most top-PIDs : many small ones
 - top SQL, with Least PIDs: repeated SQL, or long running SQL


-- regarding the metrics on ports 7000 and 9000..
- metrics are global for process of t-server/master (but what about the postgres processes): 
- postgres-processes, client-connetions: local, need more precise info if avaiable 
  client-connections, are the "most interesting" to collect metrics from  ? 

- prometheus data: many nrs, 90% in my case, are 0 ( I checked with awk...)
- even from  non-zero numbers, a lot is "available", but not necesarily "useful".
- try asking questions... how many client-conn.. how many top-queries, how many dependent qries..
- can we measure "inside a session" ?  or have a session(process, pid) save/report its metrics ?
  And even if we can peek inside processes or structures: is the data relevant + useful?
- the pg_stat_stmnts are "aggregates" over all sessions that did the query
  But the SEssion, the Client, needs to know what he was waiting for. 
- Ideally, some timing per root-request is wanted ? 
    (can be derived, via min/max of sample time, but with single-second accuracy?)
- many metrics have count + time.. but always cumulative.. 
- is there count of nr of outgoing calls per session or query? 
- how can a session report time-spent, possibly broken down to time-spent-per-root-request, or per qury..

- ASH wise: the 1sec interval seems to miss lot of short/fast queries.. 
  is pg_stat_% a better place to look for metris ? 

curl from ports : do_curls.sh..
set "|" as separator: cat node2_9000.out | sed 's/{/\|/g' | sed 's/} /\|/g' | sed 's/ /\|/g'
filter out the zeros: awk -F'|' '$3 != 0 {print $1, $3 }'

Big Question : which metrics are relevant to session or to root-request.. 


Regarding Datamodel... 

Aim is currently to find heavy processes and heavy queries.
Spotting underlying tabblets / tables may/should follow from that.
But other may be more interested to dive directly into the storage-layer
(as thew metrics on port 9000 seem to focus on rocksdb)


Central, I think, is the "session". identified from from pg_stat_activity

Session, a postgres-process with a pid on a host, belongs to a tserver, on a host
Session also links to an "application" or some component that is initiating work on the system.
table: ybx_sess_mst: try to catch the master record for every session.

My main source for session-data is pg_stat_activity.

The Session is then linked to ASH, where the work is "polled" from yb_act_sess_hist:
table: ybx_ashy_log : data logged from yb_active_session_history

ASH records are local to a tserver, 
and belong to a session or parent_process, possibly via root-request
But that RR is hard to "spot": does it belong to the Session, to the tsrver, or to the query? 
possibly the link of the three. TBD.

On the Sessions, We can regularly "poll" logging data out of a session from pg_stat_activity: 
table : ybx_sess_log, regular polled data from pg_stat_activity


Looking upwards from session, the session is initiated at a t-server, 
and we can regularly capture (scrape) data from tserver 
via yb-admin and function yb_server_metrics(). And also via port 9000/prometeus
table: ybx_tsrv_mst : 1 record per tserver
table: ybx_tsrv_log : regular polled data, e.g. memory, threads, cpu...

Similarly, we can capture data for universe and yugabyte-master
table: ybx_mast_mst: 1 record per master
table: ybx_mast_log: polled data from yb-admin

Back to Sessions, and queries.
Both the session pg_stat_activity) and the ASH-record contain a query_id.
We can capture query-data from pg_stat_statements, resulting in :
ybx_qury_mst : just signal the existance of the qry, and act as masater j+ linking-pin
ybx_qury_log : capture data per t-server from pg_stat_statements

note that the cummulative data in pg_stat_stmt is not relatable to a session (yet)

Possibly there is a query-plan (explain plan) in there, 
and a query can have muliple of those plans
table: ybx_qury_pln : some as yet unknown way to store a plan...


For storage details, we know ASH acts on tablets, local to the node where the ash is 
ybx_tblt_mst : 1 record per tablet (which will have at least 3replicas... )
ybx_tblt_rep : replica, local to a t-server, 
Note that a tablet_replicat will have "role": leader or follower or... 

Tablets are linked to tables, and becasue of possible "colocation" 
this will require an n:n. Modelling this out: table  + linking entity:
ybx_tabl_mst : master record per table, key is table_uuid, and oid links back to postgres.
ybx_tbl_log  : info from pg_stat_tables, per tsrve3r, per log_dt ?
ybx_tata_lnk : linking table to tablet: just two keys ?

tables will belong to a database, and from pg_database views we can collect info on those:
ybx_datb_mst : 1 record per database, key is datid, the oid inside postgres.
ybx_datb_log : regular polled data, per t-server key is datid + tsrverk + log_dt

The whole reasoning leads to an ERD, and to a primitive script to create the tables.
in real-life, the constraints may have to be left-out 
because of the order in which we can collect the data: details first, meta-data to be derived.

script: attached.
two partial erds: attached..

scripts and functions to collect the data: still in messy development. partly working.

graphana example.. attached.



Request to add Event_ID to the views yb_active_session_history and to yb_event_desc.

The view yb_active_session_history is useful, but in some cases we want to "keep that history" and store it into a table.

There are four text-fields: wait_event, _type, _class and _component.
For storing the data, having 4 text-fields, in 1000s or millions of records, is less efficient. 
Furthermore, from saving a few days of ASH-records, we found there are probably less than 100 unique combinations of 
event, type, class, and component. 
This was confirmed by yb_wait_ev_desc: 47 records in our case. 
Even if multiplied by the possibilities for "wait_event_component" this remains a manageable nr.
Hence the suggestion to add the ID field to both views.
A simple "int" would suffice for storing the ASH data into a table, saving on storage, memory and network-traffic. 

It will therefore be useful if the view yb_wait_event_desc 
will display an ID (wait_ev_id) that can be referred by the records in yb_wait_event_descr.

Note1 that further savings in the size of (storing) the ASH record are possible; a lot of client- and root-req-related info seems to repeat and seems to depend solely on the Root_Req. Separate issue to follow.


Root_request_id.
I am trying to figure out myself what the rr_id actually "means". 

The view yb_active_session_history contains a numbrer of fields that seem to depend on the Root_req_id.



