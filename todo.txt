
generic todo list

 - remove nodes from cluster, tservers and masters
 - split ash-script into separate repostiories on github
 - investigate multi-region and replication.
 - log an aas; avg-act-ses, find baseload ? 
 - create subdir with sh-scripts to run evey N seconds: ashloop, unames, master-log, tserver-log...

todo:
 - detect new combinations for ybx_mast_mst and ybx_tsrv_mst : 
 - has to act as decoding between uuid and host.
    - snap_id ( = also found_dt )
    - host (where it runs, not where it is found!)
    - tsrv_uuid.
    - pk: host + tsrv , the snap_id is  just a time-indicator.

next
 - log some server-data: use script, function.. use unames + yb_func().. just log /180sec
 - set grafana open
 - run some load..
 - edit scripts + instructions to deploy:
    - yb_init.sql
    - mk_ybash.sql
      - do_stuff.sh, do_ashloop.sh, do_sadc.sh
    - yb_osmetrics.sql
    - unames.sql + sh (all nodes)
    - do_snap.sh (only 1 node, needs separate loop-script.) 

need a way to read key=values into table ybx_ingest: (id-seq, host, key, value )... all text..
that way, reading from varz or just files is easier..
=> done in unames.sql

to pick up OS data: unames.sh
to pick up disk usage echo Disk_usage=`du -sm /root/var`
to pick up memory: curl -s $HOSTNAME:7000/memz?raw
to pick up memory: curl -s $HOSTNAME:9000/memz?raw
some load-info from top: top -n1 | head -n4

get master info: 
ybmast | expand | tail -n3 | sed 's/ \+/\|/g' 


To log master-server config, have 1 node collect:: 
1. determine snap_id + log_dt
2. scrape master-info:  uuid + node + port + STATUS + leader/follower
3. scrape tserver-info:  uuid + node + port + heartbeat + STATUS + leader/follower
4. log tserver_metrics for all nodes yb_server_metrics()

on each node: collect t-server-info
4. add to uname or ashloop: use last snapshot to find uuid for node, use uuid to select from yb_heap_stats()

echo "local_tserver=" `ybmast | grep $HOSTNAME | cut -c1-33`


-- work on here.. ybx_tsrv_log (how to find uuid?)
other sources of data: for tserver

-- heap is local to server, must be in host_log, via unames.sql
select * from yb_heap_stats();  

-- metrics can be part of snapshot.. 
select * from yb_servers_metrics () ;

select tl.host, tm.status, tm.error
, (tm.metrics::json->>'memory_free')::bigint/1024/1024      as mem_free_mb
, (tm.metrics::json->>'memory_total')::bigint/1024/1024     as mem_total_mb
, (tm.metrics::json->>'memory_available')::bigint/1024/1024 as mem_avail_mb
, (tm.metrics::json->>'tserver_root_memory_soft_limit')::bigint/1024/1024 as tserver_root_memory_soft_limit_mb
, (tm.metrics::json->>'tserver_root_memory_consumption')::bigint/1024/1024 as tserver_root_memory_consumption_mb
, (tm.metrics::json->>'cpu_usage_user')::real  as cpu_usage_user 
, (tm.metrics::json->>'cpu_usage_system')::real  as cpu_usage_system 
, tm.uuid
from ybx_tsrv_log tl,  yb_servers_metrics () tm
where 1=1 
-- and tl.snap_id = 4801
and tl.tsrv_uuid = tm.uuid
order by tl.host, tl.snap_id desc ; 


select * from pg_catalog.yb_mem_usage_sql() ;

next; log data and display in graphana.
:


todo - searching for SQL.
 - top-SQL with count-events.., count per SQL or TSrv
 - top SQL per node (with count-events)
 - top SQL, with most top-PIDs : many small ones
 - top SQL, with Least PIDs: repeated SQL, or long running SQL

-- metrics on ports..
- metrics are global for process of t-server (but what about the postgres processes): 
  they are client-connections, and thus "most interesting" to collect metrics from 
- many, 90%, of nrs are 0
- even from  non-zero numbers, a lot is "available", but not necesarily "useful".
- try asking questions... how many client-conn.. how many top-queries, how many dependent qries..
- can we measure "in a session" ? 
- even the pg_stat_stmnts are "aggregates" over all sessions that did the query
  But the SEssion, the Client, needs to know what he was waiting for. 
- Ideally, some timing per root-request is wanted ?
- is there count of nr of outgoing calls per session or query? 
- many metrics have count + time.. but always cumulative.. 
- how can a session report time-spent, possibly broken down to time-spent-per-root-request, or per qury..

curl from ports : do_curls.sh..
set "|" as separator: cat node2_9000.out | sed 's/{/\|/g' | sed 's/} /\|/g' | sed 's/ /\|/g'
filter out the zeros: awk -F'|' '$3 != 0 {print $1, $3 }'

Big Question : which metrics are relevant to session or to root-request.. 

