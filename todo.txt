
generic todo list

 - remove nodes from cluster, tservers and masters
 - split ash-script into separate repostiories on github
 - investigate multi-region and replication.
 - log an aas; avg-act-ses, find baseload ? 
 - create subdir with sh-scripts to run evey N seconds: ashloop, unames, master-log, tserver-log...

todo:
 - detect new combinations for ybx_mast_mst and ybx_tsrv_mst : 
 - has to act as decoding between uuid and host.
    - snap_id ( = also found_dt )
    - host (where it runs, not where it is found!)
    - tsrv_uuid.
    - pk: host + tsrv , the snap_id is  just a time-indicator.

next
 - log some server-data: use script, function.. use unames + yb_func().. just log /180sec
 - set grafana open
 - run some load..
 - edit scripts + instructions to deploy:
    - yb_init.sql
    - mk_ybash.sql
      - do_stuff.sh, do_ashloop.sh, do_sadc.sh
    - yb_osmetrics.sql
    - unames.sql + sh (all nodes)
    - do_snap.sh (only 1 node, needs separate loop-script.) 

need a way to read key=values into table ybx_ingest: (id-seq, host, key, value )... all text..
that way, reading from varz or just files is easier..
=> done in unames.sql

to pick up OS data: unames.sh
to pick up disk usage echo Disk_usage=`du -sm /root/var`
to pick up memory: curl -s $HOSTNAME:7000/memz?raw
to pick up memory: curl -s $HOSTNAME:9000/memz?raw
some load-info from top: top -n1 | head -n4

get master info: 
ybmast | expand | tail -n3 | sed 's/ \+/\|/g' 


To log master-server config, have 1 node collect:: 
1. determine snap_id + log_dt
2. scrape master-info:  uuid + node + port + STATUS + leader/follower
3. scrape tserver-info:  uuid + node + port + heartbeat + STATUS + leader/follower
4. log tserver_metrics for all nodes yb_server_metrics()

on each node: collect t-server-info
4. add to uname or ashloop: use last snapshot to find uuid for node, use uuid to select from yb_heap_stats()

echo "local_tserver=" `ybmast | grep $HOSTNAME | cut -c1-33`


-- work on here.. ybx_tsrv_log (how to find uuid?)
other sources of data: for tserver

-- heap is local to server, must be in host_log, via unames.sql
select * from yb_heap_stats();  

-- metrics can be part of snapshot.. 
select * from yb_servers_metrics () ;

select tl.host, tm.status, tm.error
, (tm.metrics::json->>'memory_free')::bigint/1024/1024  as memory_free 
, (tm.metrics::json->>'memory_total')::bigint/1024/1024 as memory_total
, (tm.metrics::json->>'memory_available')::bigint/1024/1024 as memory_available
, (tm.metrics::json->>'tserver_root_memory_soft_limit')::bigint/1024/1024 as tserver_root_memory_soft_limit
, (tm.metrics::json->>'tserver_root_memory_consumption')::bigint/1024/1024 as tserver_root_memory_consumption
, (tm.metrics::json->>'cpu_usage_user')::real  as cpu_usage_user 
, (tm.metrics::json->>'cpu_usage_system')::real  as cpu_usage_system 
, tm.uuid
from ybx_tsrv_log tl,  yb_servers_metrics () tm
where 1=1 
and tl.snap_id = 4801
and tl.tsrv_uuid = tm.uuid
order by tl.host, tl.snap_id desc ; 


select * from pg_catalog.yb_mem_usage_sql() ;

next; log data and display in graphana.
:


todo - searching for SQL.
 - top-SQL with count-events.., count per SQL or TSrv
 - top SQL per node (with count-events)
 - top SQL, with most top-PIDs : many small ones
 - top SQL, with Least PIDs: repeated SQL, or long running SQL

